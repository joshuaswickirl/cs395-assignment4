{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS395 - Assignment 4\n",
    "### Univarite LSTMs\n",
    "\n",
    "Date: Feb 6, 2019  \n",
    "By: Joshua Eli Swick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs for all code below\n",
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] 40\n",
      "[20 30 40] 50\n",
      "[30 40 50] 60\n",
      "[40 50 60] 70\n",
      "[50 60 70] 80\n",
      "[60 70 80] 90\n"
     ]
    }
   ],
   "source": [
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Run the given code 3 times. Show all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla LSTM output 1: [[102.89862]]\n",
      "Vanilla LSTM output 2: [[102.680595]]\n",
      "Vanilla LSTM output 3: [[102.354904]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Vanilla LSTM output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Change the activation function from 'relu' to 2 others. Run 3 times, show all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla LSTM, TanH activation, output 1: [[14.410823]]\n",
      "Vanilla LSTM, TanH activation, output 2: [[12.103288]]\n",
      "Vanilla LSTM, TanH activation, output 3: [[11.201336]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # tanh activation\n",
    "    model.add(LSTM(50, activation='tanh', input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Vanilla LSTM, TanH activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla LSTM, Sigmoid activation, output 1: [[9.99916]]\n",
      "Vanilla LSTM, Sigmoid activation, output 2: [[9.784918]]\n",
      "Vanilla LSTM, Sigmoid activation, output 3: [[12.060872]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # tanh activation\n",
    "    model.add(LSTM(50, activation='sigmoid', input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Vanilla LSTM, Sigmoid activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Run give code 3 times. Show all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked LSTM, output 1: [[102.345375]]\n",
      "Stacked LSTM, output 2: [[102.31267]]\n",
      "Stacked LSTM, output 3: [[103.0182]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True,\n",
    "    input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Stacked LSTM, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "Change the activation function from 'relu' to 2 others. Run 3 times, show all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked LSTM, TanH activation, output 1: [[19.52164]]\n",
      "Stacked LSTM, TanH activation, output 2: [[19.069542]]\n",
      "Stacked LSTM, TanH activation, output 3: [[19.668125]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='tanh', return_sequences=True,\n",
    "    input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(50, activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Stacked LSTM, TanH activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked LSTM, Sigmoid activation, output 1: [[12.196514]]\n",
      "Stacked LSTM, Sigmoid activation, output 2: [[10.7032795]]\n",
      "Stacked LSTM, Sigmoid activation, output 3: [[11.839983]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='sigmoid', return_sequences=True,\n",
    "    input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(50, activation='sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Stacked LSTM, Sigmoid activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "What are the primary differences between the Stacked LSTM and the Vanilla LSTM? Describe and explain what is occuring differently. When would you use the Stacked LSTM instead of the Vanilla LSTM and vise versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary difference between a Stacked LSTM and a Vanilla LSTM is the number of LSTM layers. A Vanilla LSTM has only one layer, while Stacked LSTMs have more than one LSTM layer, meaning multiple memory cells.\n",
    "\n",
    "A Stacked LSTM may improve accuracy in cases where the model benefits from from learning the hierarchical representation of complex time-series data. A Vanilla LSTM is useful in less complex time series data that still have long term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "Run the given code 3 times. Show all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional LSTM, output 1: [[101.993484]]\n",
      "Bidirectional LSTM, output 2: [[102.12818]]\n",
      "Bidirectional LSTM, output 3: [[100.94301]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps,\n",
    "    n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Bidirectional LSTM, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "Change the activation function from 'relu' to 2 others. Run atleast 3 times, show outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional LSTM, TanH activation, output 1: [[26.314335]]\n",
      "Bidirectional LSTM, TanH activation, output 2: [[27.845387]]\n",
      "Bidirectional LSTM, TanH activation, output 3: [[26.081753]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(50, activation='tanh'), input_shape=(n_steps,\n",
    "    n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Bidirectional LSTM, TanH activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional LSTM, Sigmoid activation, output 1: [[22.500427]]\n",
      "Bidirectional LSTM, Sigmoid activation, output 2: [[20.091812]]\n",
      "Bidirectional LSTM, Sigmoid activation, output 3: [[21.567383]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(50, activation='sigmoid'), input_shape=(n_steps,\n",
    "    n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Bidirectional LSTM, Sigmoid activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "What are the primary differences between the Bidirectional LSTM and the Vanilla LSTM? Describe and explain what is occurring differently. When would you use the Bidirectional LSTM instead of the Vanilla\n",
    "LSTM and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary difference between a Bidirectional LSTM and a Vanilla LSTM is that a Bidirectional LSTM run the inputs both from past to future and future to past, preserving information from both the past and the future. Vanilla LSTMs only preserve information from the past.\n",
    "\n",
    "Bidirectional LSTMs may improve accuracy in models where context are important such as predicting latter part of a sentence in language.\n",
    "A Vanilla LSTM may be more appropriate in less complex models where the context is less related to the future data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "Run the given code 3 times. Show all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN LSTM, output 1: [[102.702675]]\n",
      "CNN LSTM, output 2: [[103.33146]]\n",
      "CNN LSTM, output 3: [[103.06571]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1,activation='relu'),\n",
    "                              input_shape=(None, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([60, 70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"CNN LSTM, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.\n",
    "Change the activation function from 'relu' to 2 others. Run 3 times, show all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN LSTM, TanH activation, output 1: [[31.176186]]\n",
      "CNN LSTM, TanH activation, output 2: [[29.775518]]\n",
      "CNN LSTM, TanH activation, output 3: [[31.138159]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1,activation='tanh'),\n",
    "                              input_shape=(None, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(50, activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([60, 70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"CNN LSTM, TanH activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN LSTM, Sigmoid activation, output 1: [[18.309414]]\n",
      "CNN LSTM, Sigmoid activation, output 2: [[20.266657]]\n",
      "CNN LSTM, Sigmoid activation, output 3: [[16.436787]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1,activation='tanh'),\n",
    "                              input_shape=(None, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(50, activation='sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([60, 70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"CNN LSTM, Sigmoid activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.\n",
    "What are the primary differences between the CNN LSTM and the Vanilla LSTM? Describe and explain what is occurring differently. When would you use the CNN LSTM instead of the Vanilla LSTM and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary difference between a CNN LSTM and a Vanilla LSTM is that CNN LSTMs have a Convolutional Neural Network component.\n",
    "\n",
    "CNN LSTM models improve accuracy in 2D models related to spatial inputs, like images and videos. Similar datasets, comprised of spatial data points, are not easily modeled using Vanilla LSTMs. Vanilla LSTMs should be used in simpler problems where a vanishing gradient is still a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.\n",
    "Run the given code 3 times. Show all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv-LSTM, output 1: [[103.641975]]\n",
      "Conv-LSTM, output 2: [[104.15526]]\n",
      "Conv-LSTM, output 3: [[104.22724]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', \n",
    "                         input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([60, 70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Conv-LSTM, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. \n",
    "Change the activation function from 'relu' to 2 others. Run 3 times, show all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv-LSTM, TanH activation, output 1: [[23.881386]]\n",
      "Conv-LSTM, TanH activation, output 2: [[25.381102]]\n",
      "Conv-LSTM, TanH activation, output 3: [[26.844866]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='tanh',\n",
    "                         input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([60, 70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Conv-LSTM, TanH activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv-LSTM, Sigmoid activation, output 1: [[21.401457]]\n",
      "Conv-LSTM, Sigmoid activation, output 2: [[16.453602]]\n",
      "Conv-LSTM, Sigmoid activation, output 3: [[19.233423]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='sigmoid',\n",
    "                         input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = array([60, 70, 80, 90])\n",
    "    x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    print(f\"Conv-LSTM, Sigmoid activation, output {run+1}: {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.\n",
    "What are the primary differences between the Conv‐LSTM and the CNN LSTM? Describe and explain what is occurring differently. When would you use the Conv‐LSTM instead of the CNN LSTM and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary difference between a Conv-LSTM and a CNN LSTM is the ConvLSTM has Convolutional Neural Network directly within the LSTM, where the CNN LSTM is an integration between a CNN and an LSTM. The CNN layers output a 1-D result to the LSTM part of the model.\n",
    "\n",
    "A Conv-LSTM is designed to take 3D input data, which is appropriate for problems that include video or radar image data.\n",
    "A CNN LSTM is appropriate for 2D input data such as images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.\n",
    "Provide a single table that illustrates the different LSTM models, the activation functions used, and the average of the 3 results (predictions) you received to 3 decimal places. What are your general findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| LSTM Model | Activation Function Used | Avg. of 3 Results | General Findings |\n",
    "| ---------- | ------------------------ | ----------------- | ---------------- |\n",
    "| Vanilla    | relu                     | 102.645 ||\n",
    "| Vanilla    | tanh                     | 012.572 ||\n",
    "| Vanilla    | sigmoid                  | 010.615 ||\n",
    "| Stacked    | relu                     | 102.559 ||\n",
    "| Stacked    | tanh                     | 019.420 ||\n",
    "| Stacked    | sigmoid                  | 011.580 ||\n",
    "| Bidirectional | relu                  | 101.688 ||\n",
    "| Bidirectional | tanh                  | 026.747 ||\n",
    "| Bidirectional | sigmoid               | 021.386 ||\n",
    "| CNN        | relu                     | 103.034 ||\n",
    "| CNN        | tanh                     | 030.697 ||\n",
    "| CNN        | sigmoid                  | 018.338 ||\n",
    "| Conv       | relu                     | 104.008 ||\n",
    "| Conv       | tanh                     | 025.370 ||\n",
    "| Conv       | sigmoid                  | 019.029 ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the LSTM model used was less impactful than the activation function, where RelU was by far the most accurate.\n",
    "\n",
    "Comparing the LSTM models using RelU activation function the most accurate models where, in order by decreasing accuracy: Bidirectional, Stacked, Vanilla, CNN, Conv.\n",
    "\n",
    "The Bidrectional LSTM model being the most accurate is to be expected as that model is learning from the context of the data in relation to the \"future\" data, increasing the accuracy of the contextual prediction. The "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
